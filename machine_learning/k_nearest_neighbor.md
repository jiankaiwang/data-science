# K nearest neighbor

<script src="../../js/general.js"></script>

* KNN 為一常用的分類方法，概念為 : 
  1. 找出距離某點最近的 K 個鄰近點，而成一個類別。
  2. KNN 中各點的專家定義特徵，可以透過平均方式或是加權方式計算，而成為此類別的特徵値。

###Training dataset
---

* 測試組模擬資料，用來告訴 KNN 分類器如何分辨每一個資料點。

```text
label          1           2           3           4           5           6           7           8           9          10
2.84  0.58333333  0.66666667  0.91666667  0.91666667  0.58333333        0.75  0.66666667  0.58333333  0.66666667        0.75
1.59  0.66666667  0.66666667        0.75        0.75  0.66666667  0.66666667  0.66666667  0.58333333  0.58333333        0.75
0.92  0.66666667        0.75        0.75        0.75        0.75  0.66666667  0.83333333  0.66666667        0.75  0.66666667
1.2         0.75        0.75  0.66666667        0.75        0.75  0.66666667  0.66666667  0.66666667        0.75  0.66666667
1.66  0.66666667  0.66666667        0.75  0.83333333  0.66666667  0.66666667  0.66666667  0.66666667  0.66666667        0.75
1.62  0.66666667  0.66666667        0.75        0.75        0.75  0.66666667  0.66666667  0.58333333  0.66666667        0.75
1.39  0.66666667  0.66666667        0.75  0.66666667  0.66666667  0.66666667  0.66666667  0.58333333  0.58333333  0.66666667
2.56  0.58333333  0.66666667  0.83333333  0.83333333  0.66666667        0.75  0.58333333  0.58333333  0.58333333  0.83333333
0.23        0.75  0.66666667  0.66666667  0.66666667  0.83333333  0.66666667        0.75        0.75        0.75  0.66666667
```

###Testing dataset
---

* 用來模擬 KNN 分類器分類新遇到每一個點的分類結果。

```text
label           1           2           3           4           5           6           7           8           9          10
    1        0.75        0.75        0.75  0.83333333  0.58333333        0.75  0.66666667  0.58333333  0.66666667  0.66666667
    1  0.66666667        0.75        0.75        0.75  0.58333333  0.66666667        0.75  0.66666667  0.58333333  0.66666667
    1  0.66666667  0.66666667  0.83333333        0.75  0.58333333  0.66666667  0.66666667  0.66666667  0.66666667  0.66666667
    1  0.66666667  0.58333333  0.66666667        0.75  0.66666667  0.66666667  0.66666667  0.66666667        0.75  0.66666667
    1  0.58333333        0.75  0.83333333        0.75  0.66666667  0.66666667  0.66666667  0.58333333  0.58333333  0.66666667
    1  0.66666667  0.66666667  0.83333333  0.83333333  0.66666667  0.66666667        0.75        0.75  0.66666667  0.66666667
    1  0.66666667  0.66666667        0.75        0.75  0.66666667        0.75  0.66666667  0.66666667  0.66666667  0.66666667
    1  0.83333333        0.75  0.83333333        0.75  0.66666667  0.66666667        0.75        0.75        0.75  0.66666667
    1  0.58333333        0.75  0.66666667        0.75  0.58333333        0.75        0.75  0.58333333  0.66666667  0.58333333
```

###KNN in R
---

* KNN 是透過多個分類特徵來建立起分類依據，但並非所有特徵皆須納入考慮，可以透過簡單的資料分散性來取出資料較為分散的特徵，並透過訓練此篩選出的特徵進行分類訓練即可。

```R
library("class")

# read data
getOriData <- read.table("training.12-mer.features.csv",header=T,sep=",")
getTest <- read.table("testing.12-mer.features.csv",header=T,sep=",")

#-------------------------------------------
# data distribution to select dispersible data features
sdData <- function(data) {
	return(sd(data))
}

getDataDistribution <- apply(getOriData[,-1],2,sdData)

# sort
getMaxMin <- as.matrix(sort(getDataDistribution))
#--------------------------------

# selection range
lRange <- which(getMaxMin[] > 0.062)
sRange <- which(getMaxMin[] < 0.043)

# total
getMaxCol <- rownames(getMaxMin)[lRange]
getMinCol <- rownames(getMaxMin)[sRange]
getTtlFeatures <- c(getMaxCol,getMinCol)

# get for trainning
train <- getOriData[,getTtlFeatures]
labels <- getOriData[,"label"]
test <- getTest[,getTtlFeatures]

# for knn function
classRes1 <- knn(train,test,labels,k = 3, l = 0, prob = FALSE, use.all = TRUE)

# contain KNN result
write.table(classRes1,"output1_1.txt",sep=",",quote=FALSE,eol = "\n",row.names = FALSE,col.names= FALSE)
```