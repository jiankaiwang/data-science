# Random Forest

<script src="../js/general.js"></script>

###Discussion and flows
---

Random Forest is a **divide-and-conquer** method useful for data classification and 
regression. Random Forest is an **ensemble learning** method - generate several classifiers and 
aggregate their results into one at final - built on **decision trees** (figure.1 [1]). In the other 
words, a model generated by random forest combines several **weaker learners** (classifiers) into 
a **stronger learner** at final. The following figure.2 [1] is an example of the above statements. 

![](../images/rf_decision-tree.png)

[Figure.1] A simple example demonstrates decision trees. There is a variable with conditions to classify two dimension data, people who want to play and not to play ball games. 

![](../images/rf_random_forest.png)

[Figure.2] The basic concept implements random forest method. The blue point represents each observed data. Each gray curve, a weaker learner, is an approximation to one subset of total data. The red curve, a stronger learner, is much better and outstanding than the other gray curve. 

Basically about **66% (2/3)** of total data in random would be taken as a training dataset 
building decision trees and the other **34% (1/3)** of total data would be regard as a testing 
dataset. There are two methods specific to implement combining classification trees, **boosting 
and bagging**. In boosting tree (figure.3 [2]), successive trees would modify and give an extra 
weighted score to points which were incorrectly predicted by previous trees. After each data is 
taken at least once and the predicting result is converged, the action of random selecting and 
construction of decision trees would be stopped. In the end, a vote with weighted would be 
used for prediction. In bagging tree (figure.4 [2]), successive trees would be operated 
independently from the previous trees. That is, each time decision tree construction would use 
a bootstrap sample of the dataset. After each data is selected once, a majority vote is used for 
prediction. 

![](../images/rf_Boosting-of-tree-construction.png)

[Figure.3] Boosting of tree construction. Successive trees would get weighted to points which were incorrectly predicted. In the end prediction would be accomplished by taking a vote with weighted. 

![](../images/rf_Bagging-of-tree-construction.png)

[Figure.4] Bagging of tree construction. Successive trees would operate independently from the previous tree. In the end prediction would be accomplished by a simple majority vote. 

There are many methods using random forest concept as their core. In data mining and 
machine learning, random forest method would be used for classifying data and for regression 
of data.   

The random forest method is famous for operative speed, unbalanced data and missing 
data. But its weakness is hard to predict beyond the range in the training data and could be 
over-fitting when data with higher noise exists. 

###Example in R
---

* Use R to implement random forest prediction (classification) and regression











